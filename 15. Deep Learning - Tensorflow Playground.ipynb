{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Deep Learning -  Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Welcome to this module of Deep Learning!\n",
    "\n",
    "üéØ In this challenge, our goal is two-fold:\n",
    "1. Get a visual representation of Neural Networks\n",
    "2. Build a better intuition of what Neural Networks are doing\n",
    "\n",
    "üëâ We will use ***[Tensorflow Playground](https://playground.tensorflow.org/)***\n",
    "\n",
    "_(This first challenge does not require much coding_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Let's go to the [Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=2&seed=0.23545&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&regularization_hide=true&showTestData_hide=false&stepButton_hide=false&activation_hide=false&problem_hide=false&batchSize_hide=true&dataset_hide=false&resetButton_hide=false&discretize_hide=false&playButton_hide=false&learningRate_hide=true&regularizationRate_hide=true&percTrainData_hide=false&numHiddenLayers_hide=false) and select the following type of data ‚ùì \n",
    "\n",
    "- A classification problem \n",
    "- The circle dataset (<span style=\"color:blue\">blue dots</span> inside a circle of <span style=\"color:orange\">orange dots</span>)\n",
    "- Ratio of training to test data : $ 70 \\% $\n",
    "- No noise ($ = 0$)\n",
    "- Do not show test data (right panel) \n",
    "- Do not discretize the output\n",
    "- Activation function: ***ReLU*** \n",
    "\n",
    "<details>\n",
    "    <summary><i> Why Relu? </i></summary>\n",
    "        \n",
    "üí° In general, try it by default. It appears to work better for many problems!\n",
    "    \n",
    "_Note: Playground only allows you to select **one** activation function that is used for **all** of the **hidden** layers_\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) The features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì <u>Questions about the features</u> ‚ùì\n",
    "\n",
    "1. Select only the features $X_1$ and $X_2$ (_unselect the other features if necessary_)\n",
    "2. If you were using the other variables such as $X_1^{2}$, $X_2^{2}$, $X_1 X_2$, $sin(X_1)$ and $sin(X_2)$, what type of classic Machine Learning operation does it correspond to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "* It corresponds to some type of ***feature engineering*** where you transform them. \n",
    "    * <i>Examples: multiplication, sinus, square, ...</i>\n",
    "* Here, in this exercise but also tomorrow, we will only use the raw input features $X_1$ and $X_2$. \n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Building and Fitting a Neural Network in ***Playground***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì <u>Questions about Neural Networks</u> ‚ùì \n",
    "\n",
    "* üß† Build a model with the following architecture:\n",
    "    - three hidden layers\n",
    "    - 5 neurons on the first hidden layer\n",
    "    - 4 neurons on the second hidden layer\n",
    "    - 3 neurons on the last hidden layer\n",
    "    - In ***Playground***, the output layer is not represented: \n",
    "        - For such binary classification task, keep in mind that it will automatically be a dense layer with 1 neuron activated by the sigmoid function $ \\large \\phi(z) = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "* üí™ ***Fit it and stop the iterations when the loss function has stabilized.***\n",
    "\n",
    "* üëÄ Observe carefully:\n",
    "    - Look at the individual neurons and try to understand what each neuron has specialized for during the _.fit()_\n",
    "    - What do you think about the overall shape your results? Re-run the neural network with different activation functions to compare. Can you make it work with \"Linear\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Answer: some insights about the activation functions</summary>\n",
    "\n",
    "- Results may look like a hexagon because ReLu is piece-wise linear!\n",
    "- A non-linearly separable problem cannot be fitted with a linear activation such as **Linear**\n",
    "- Surprisingly, a piece-wise linear activation function such as **ReLu** (or **LeakyReLu**) fits this non-linearly separable problem well (even if that is not always true)\n",
    "- The `tanh` activation gives a \"smoother\" decision boundary\n",
    "- The **sigmoid** does **not** seem to work well here.\n",
    "    \n",
    "üßëüèª‚Äçüè´ Always start with ReLu, it's a safe bet üßëüèª‚Äçüè´!\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Building and Fitting a Neural Network in ***Tensorflow.Keras***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá We wrote the same model for you - at least the architecture - in Tensorflow's Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(5, activation='relu', input_dim=2)) # 1st hidden layer with 5 neurons\n",
    "model.add(layers.Dense(4, activation='relu')) # 2nd hidden layer with 4 neurons\n",
    "model.add(layers.Dense(3, activation='relu')) # 3rd hidden layer with 3 neurons\n",
    "\n",
    "\n",
    "\n",
    "model.add(layers.Dense(1, activation='sigmoid')) # Output layer that outputs a probability of belonging\n",
    "                                                 # to the class of \"success\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>What to understand about the code of a Neural Network? </i>üëÜ</summary>\n",
    "\n",
    "- <u>First Hidden Layer a.k.a ***Input Layer***</u>:\n",
    "    - Every datapoint that will be input to the neural network has two features $ X = \\begin{bmatrix} \n",
    "           X_{1} \\\\\n",
    "           X_{2} \\\\\n",
    "         \\end{bmatrix} $.\n",
    "    - You need to inform your Neural Network about the ***number of input features*** through the ***`input_dim` argument***\n",
    "    - A Neural Network tries to mimic the human brain. Here we would like to use 5 neurons to start analyzing each of these points.\n",
    "    \n",
    "    - Every datapoint goes through the first hidden layer which was built using 5 neurons $ layer_1 = \\begin{bmatrix} \n",
    "           a_{1} \\\\\n",
    "           a_{2} \\\\\n",
    "           a_{3} \\\\\n",
    "           a_{4} \\\\\n",
    "           a_{5} \\\\           \n",
    "         \\end{bmatrix} $\n",
    "    \n",
    " - <u>Second Hidden Layer</u>:\n",
    "         \n",
    "    - What if we want to ***make the information flow*** through a second hidden layer with 4 neurons? It is totally possible!\n",
    "    - These 4 neurons $ layer_2 = \\begin{bmatrix} \n",
    "           b_{1} \\\\\n",
    "           b_{2} \\\\\n",
    "           b_{3} \\\\\n",
    "           b_{4} \\\\ \n",
    "         \\end{bmatrix} $ from the second layer will analyze the output from the 5 neurons in the first layer\n",
    "    \n",
    "- <u>Third Hidden Layer</u>:\n",
    "        - What if we want the information to **continue to flow** through a third hidden layer with 3 neurons? Again, totally possible!\n",
    "\n",
    "    - Every neuron's output from the second layer goes through the third hidden layer which was built using 3 neurons $ layer_3 = \\begin{bmatrix} \n",
    "           c_{1} \\\\\n",
    "           c_{2} \\\\\n",
    "           c_{3} \n",
    "         \\end{bmatrix} $\n",
    "         \n",
    "    - These 3 neurons analyze the outputs of the neurons in $ layer_2  $ !\n",
    "\n",
    "- <u>Predictive Layer</u>\n",
    "    - You are dealing with a binary classification task\n",
    "    - We could use two neurons to predict the probability of belonging to class A or class B...\n",
    "    - But one neuron predicting the probability of \"success\" is enough\n",
    "\n",
    "- <u>About activation functions</u>\n",
    "    - Despite its simplicity, the ***ReLU*** has proven to be very effective to add some non-linearity to the layers\n",
    "    - For the predictive layer, the best activation function to use for a classification task is the ***sigmoid*** function. That is something we've already discussed during Decision Science and Machine Learning.\n",
    "\n",
    "- <u>About the Sequential aspect of the Network</u>:\n",
    "    - The fact that you are defining a **Sequential** model has a consequence: each layer is aware of its input size based on the output size of the previous layer!\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì How many parameters are involved in this small Neural Network ‚ùì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculons les param√®tres pour votre r√©seau neuronal :\n",
    "\n",
    "1√®re couche cach√©e : (2+1)√ó5=15 param√®tres\n",
    "2√®me couche cach√©e : (5+1)√ó4=24 param√®tres\n",
    "3√®me couche cach√©e : (4+1)√ó3=15 param√®tres\n",
    "Couche de sortie : (3+1)√ó1=4(3+1)√ó1=4 param√®tres\n",
    "\n",
    "En sommant les param√®tres de toutes les couches : 15+24+15+4=58\n",
    "\n",
    "Donc, il y a 58 param√®tres dans ce petit r√©seau neuronal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Hint</i></summary>\n",
    "\n",
    "‚úÖ You should have 58 parameters\n",
    "    \n",
    "‚ùå If not, double-check your architecture    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) The XOR Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì <u>Playing with the XOR Dataset</u> ‚ùì \n",
    "\n",
    "* On Playground:\n",
    "    - Change the dataset to the \"XOR - Exclusive Or\".\n",
    "    - Try to design a model with two hidden layers that has a very small **test loss** \n",
    "        - Note: you are free to choose the number of neurons per layer yourself.  \n",
    "        \n",
    "* Coding with Tensorflow/Keras:\n",
    "    - Once you have built your model on Playground, code it down below with the Tensorflow/Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.6848 - accuracy: 1.0000 - val_loss: 0.6844 - val_accuracy: 0.7500\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6844 - accuracy: 0.7500 - val_loss: 0.6840 - val_accuracy: 0.7500\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6840 - accuracy: 0.7500 - val_loss: 0.6836 - val_accuracy: 0.7500\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.6836 - accuracy: 0.7500 - val_loss: 0.6832 - val_accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6832 - accuracy: 1.0000 - val_loss: 0.6828 - val_accuracy: 1.0000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.6828 - accuracy: 1.0000 - val_loss: 0.6824 - val_accuracy: 0.7500\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.6824 - accuracy: 0.7500 - val_loss: 0.6820 - val_accuracy: 0.7500\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 0.6820 - accuracy: 0.7500 - val_loss: 0.6816 - val_accuracy: 0.7500\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 0.6816 - accuracy: 0.7500 - val_loss: 0.6812 - val_accuracy: 0.7500\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.6812 - accuracy: 0.7500 - val_loss: 0.6807 - val_accuracy: 0.7500\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6807 - accuracy: 0.7500 - val_loss: 0.6803 - val_accuracy: 0.7500\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.6803 - accuracy: 0.7500 - val_loss: 0.6799 - val_accuracy: 0.7500\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.6799 - accuracy: 0.7500 - val_loss: 0.6795 - val_accuracy: 0.7500\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6795 - accuracy: 0.7500 - val_loss: 0.6791 - val_accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 130ms/step - loss: 0.6791 - accuracy: 0.7500 - val_loss: 0.6786 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.6786 - accuracy: 1.0000 - val_loss: 0.6782 - val_accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6782 - accuracy: 1.0000 - val_loss: 0.6778 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6778 - accuracy: 1.0000 - val_loss: 0.6774 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.6774 - accuracy: 1.0000 - val_loss: 0.6769 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6769 - accuracy: 1.0000 - val_loss: 0.6765 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6765 - accuracy: 1.0000 - val_loss: 0.6761 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6761 - accuracy: 1.0000 - val_loss: 0.6756 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6756 - accuracy: 1.0000 - val_loss: 0.6752 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6752 - accuracy: 1.0000 - val_loss: 0.6747 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6747 - accuracy: 1.0000 - val_loss: 0.6743 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6743 - accuracy: 1.0000 - val_loss: 0.6738 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6738 - accuracy: 1.0000 - val_loss: 0.6734 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6734 - accuracy: 1.0000 - val_loss: 0.6729 - val_accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6729 - accuracy: 1.0000 - val_loss: 0.6725 - val_accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6725 - accuracy: 1.0000 - val_loss: 0.6720 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6720 - accuracy: 1.0000 - val_loss: 0.6716 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6716 - accuracy: 1.0000 - val_loss: 0.6711 - val_accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.6711 - accuracy: 1.0000 - val_loss: 0.6706 - val_accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6706 - accuracy: 1.0000 - val_loss: 0.6702 - val_accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6702 - accuracy: 1.0000 - val_loss: 0.6697 - val_accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6697 - accuracy: 1.0000 - val_loss: 0.6692 - val_accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6692 - accuracy: 1.0000 - val_loss: 0.6687 - val_accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6687 - accuracy: 1.0000 - val_loss: 0.6684 - val_accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6684 - accuracy: 1.0000 - val_loss: 0.6682 - val_accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6682 - accuracy: 1.0000 - val_loss: 0.6679 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6679 - accuracy: 1.0000 - val_loss: 0.6675 - val_accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6675 - accuracy: 1.0000 - val_loss: 0.6671 - val_accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6671 - accuracy: 1.0000 - val_loss: 0.6667 - val_accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.6667 - accuracy: 1.0000 - val_loss: 0.6662 - val_accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.6662 - accuracy: 1.0000 - val_loss: 0.6657 - val_accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6657 - accuracy: 1.0000 - val_loss: 0.6655 - val_accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6655 - accuracy: 1.0000 - val_loss: 0.6653 - val_accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.6653 - accuracy: 1.0000 - val_loss: 0.6650 - val_accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6650 - accuracy: 1.0000 - val_loss: 0.6646 - val_accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.6646 - accuracy: 1.0000 - val_loss: 0.6642 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6642 - accuracy: 1.0000 - val_loss: 0.6637 - val_accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6637 - accuracy: 1.0000 - val_loss: 0.6632 - val_accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.6632 - accuracy: 1.0000 - val_loss: 0.6628 - val_accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.6628 - accuracy: 1.0000 - val_loss: 0.6625 - val_accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6625 - accuracy: 1.0000 - val_loss: 0.6622 - val_accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6622 - accuracy: 1.0000 - val_loss: 0.6618 - val_accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6618 - accuracy: 1.0000 - val_loss: 0.6613 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6613 - accuracy: 1.0000 - val_loss: 0.6610 - val_accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6610 - accuracy: 1.0000 - val_loss: 0.6607 - val_accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6607 - accuracy: 1.0000 - val_loss: 0.6603 - val_accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6603 - accuracy: 1.0000 - val_loss: 0.6599 - val_accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6599 - accuracy: 1.0000 - val_loss: 0.6595 - val_accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.6595 - accuracy: 1.0000 - val_loss: 0.6591 - val_accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.6591 - accuracy: 1.0000 - val_loss: 0.6586 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6586 - accuracy: 1.0000 - val_loss: 0.6582 - val_accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6582 - accuracy: 1.0000 - val_loss: 0.6578 - val_accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.6578 - accuracy: 1.0000 - val_loss: 0.6574 - val_accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 0.6574 - accuracy: 1.0000 - val_loss: 0.6570 - val_accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6570 - accuracy: 1.0000 - val_loss: 0.6565 - val_accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6565 - accuracy: 1.0000 - val_loss: 0.6561 - val_accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 0.6561 - accuracy: 1.0000 - val_loss: 0.6557 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 0.6557 - accuracy: 1.0000 - val_loss: 0.6553 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.6553 - accuracy: 1.0000 - val_loss: 0.6548 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 0.6548 - accuracy: 1.0000 - val_loss: 0.6544 - val_accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.6544 - accuracy: 1.0000 - val_loss: 0.6540 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 135ms/step - loss: 0.6540 - accuracy: 1.0000 - val_loss: 0.6535 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.6535 - accuracy: 1.0000 - val_loss: 0.6531 - val_accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 0.6531 - accuracy: 1.0000 - val_loss: 0.6526 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 0.6526 - accuracy: 1.0000 - val_loss: 0.6522 - val_accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.6522 - accuracy: 1.0000 - val_loss: 0.6517 - val_accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.6517 - accuracy: 1.0000 - val_loss: 0.6513 - val_accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.6513 - accuracy: 1.0000 - val_loss: 0.6508 - val_accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.6508 - accuracy: 1.0000 - val_loss: 0.6504 - val_accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.6504 - accuracy: 1.0000 - val_loss: 0.6499 - val_accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 145ms/step - loss: 0.6499 - accuracy: 1.0000 - val_loss: 0.6495 - val_accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 0.6495 - accuracy: 1.0000 - val_loss: 0.6490 - val_accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 0.6490 - accuracy: 1.0000 - val_loss: 0.6485 - val_accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 0.6485 - accuracy: 1.0000 - val_loss: 0.6480 - val_accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 0.6480 - accuracy: 1.0000 - val_loss: 0.6475 - val_accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 0.6475 - accuracy: 1.0000 - val_loss: 0.6471 - val_accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.6471 - accuracy: 1.0000 - val_loss: 0.6466 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.6466 - accuracy: 1.0000 - val_loss: 0.6461 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 0.6461 - accuracy: 1.0000 - val_loss: 0.6456 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 147ms/step - loss: 0.6456 - accuracy: 1.0000 - val_loss: 0.6452 - val_accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 154ms/step - loss: 0.6452 - accuracy: 1.0000 - val_loss: 0.6447 - val_accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.6447 - accuracy: 1.0000 - val_loss: 0.6442 - val_accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.6442 - accuracy: 1.0000 - val_loss: 0.6438 - val_accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 169ms/step - loss: 0.6438 - accuracy: 1.0000 - val_loss: 0.6434 - val_accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 0.6434 - accuracy: 1.0000 - val_loss: 0.6430 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.6430 - accuracy: 1.0000 - val_loss: 0.6426 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x145b6f79d10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neural Network that can be well fitted to the XOR Dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# XOR dataset\n",
    "X_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_train = np.array([0, 1, 1, 0])\n",
    "\n",
    "X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_test = np.array([0, 1, 1, 0])\n",
    "\n",
    "\n",
    "# D√©finir l'architecture du mod√®le\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(8, activation='relu', input_dim=2))  # 1√®re couche cach√©e avec 8 neurones\n",
    "model.add(layers.Dense(4, activation='relu'))  # 2√®me couche cach√©e avec 4 neurones\n",
    "model.add(layers.Dense(1, activation='sigmoid'))  # Couche de sortie avec activation sigmo√Øde\n",
    "\n",
    "# Compiler le mod√®le\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Charger l'ensemble de donn√©es XOR - Vous devez fournir vos propres donn√©es XOR ici\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) The Spiral Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì <u>Playing with the Spiral Dataset</u> ‚ùì \n",
    "\n",
    "* On Playground:\n",
    "    - Change the dataset to the \"Spiral\".\n",
    "    - Try to design a model with three hidden layers that has a very small **test loss** \n",
    "        - Note: you are free to choose the number of neurons per layer yourself.  \n",
    "        \n",
    "* Coding with Tensorflow/Keras:\n",
    "    - Once you have built your model on Playground, code it down  below with the Tensorflow/Keras library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Charger l'ensemble de donn√©es Spiral - Vous devez fournir vos propres donn√©es Spiral ici\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Entra√Æner le mod√®le\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n",
      "File \u001b[1;32mc:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file518gzre2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\benoi\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Neural Network that can be well fitted to the Spiral Dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# D√©finir l'architecture du mod√®le\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_dim=2))  # 1√®re couche cach√©e avec 64 neurones\n",
    "model.add(layers.Dense(32, activation='relu'))  # 2√®me couche cach√©e avec 32 neurones\n",
    "model.add(layers.Dense(16, activation='relu'))  # 3√®me couche cach√©e avec 16 neurones\n",
    "model.add(layers.Dense(3, activation='softmax'))  # Couche de sortie avec activation softmax pour la classification √† 3 classes\n",
    "\n",
    "# Compiler le mod√®le\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Charger l'ensemble de donn√©es Spiral - Vous devez fournir vos propres donn√©es Spiral ici\n",
    "\n",
    "# Entra√Æner le mod√®le\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) How Deep should a Neural Network be ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ If you compare the number of parameters needed to fit the Spiral Dataset vs. the XOR dataset, the former requires many more weights....\n",
    "\n",
    "üòÉ Actually, if your models are deep enough, you could potentially fit pretty much any pattern...\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary><i>Should I create Very Deep Neural Networks? </i></summary>\n",
    "        \n",
    "<u>Examples:</u>\n",
    "    \n",
    "* Think about a human being. The more this person spends time coding in Python, the better he/she will get better at it!\n",
    "    \n",
    "* Think about a student. The more this person studies, the better he/she will pass exams. But sometimes students can study \"too much\" about a topic and forget about the global picture of a course....\n",
    "    \n",
    "<u>Lessons</u>\n",
    "    \n",
    "üß† For Deep Learning Models, the more layers they have, the more opportunities they will have to learn the patterns in the data...\n",
    "\n",
    "‚ùóÔ∏è The problem is about avoiding **overfitting** ‚ùóÔ∏è\n",
    "    \n",
    "‚ò†Ô∏è Add a good deal of noise and you _may_ see that your model will have learned \"too much\" about this noise. \n",
    "  \n",
    "    \n",
    "üìÜ The next lecture **Deep Learning > Optimizers, Loss, & Fitting** is dedicated to helping you understand which techniques we can use to prevent Deep Learning models from overfitting.\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details>\n",
    "    <summary><i>A picture of overfitting in Playground</i></summary>\n",
    "    \n",
    "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/DL/playground-overfitting.png' width=700 style='margin:auto'>\n",
    "</details>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Let's try to complete a Regression Task using Deep Learning</u>\n",
    "\n",
    "\n",
    "This time, the last layer will no longer look like:  \n",
    "```python\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "```\n",
    "\n",
    "but instead  :\n",
    "```python\n",
    "model.add(layers.Dense(1, activation='linear'))\n",
    "```\n",
    "\n",
    "This means that the output of this network is no longer between $0$ and $1$ (probability) but between $ -\\infty$ and $+ \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì <u>Playing with the Regression Dataset</u> ‚ùì \n",
    "\n",
    "* On Playground:\n",
    "    - Change the dataset to the \"Regression\".\n",
    "    - Try to design a model that has a very small **test loss** \n",
    "        - Note: you are free to choose both the number of layers and the number of neurons per layer yourself \n",
    "        \n",
    "* Coding with Tensorflow/Keras:\n",
    "    - Once you have built your model on Playground, code it down  below with the Tensorflow/Keras library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network that can be well fitted to the Regression Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
